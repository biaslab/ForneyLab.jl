var documenterSearchIndex = {"docs":
[{"location":"getting-started/#Getting-started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"This page provides the necessary information you need to get started with ForneyLab. We will show the general approach to solving inference problems with ForneyLab by means of a running example: inferring the bias of a coin.","category":"page"},{"location":"getting-started/#Installation","page":"Getting started","title":"Installation","text":"","category":"section"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"Install ForneyLab through the Julia package manager:","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"] add ForneyLab","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"note: Note\nIf you want to use the graph visualization functions, you need to have GraphViz installed.","category":"page"},{"location":"getting-started/#Example:-Inferring-the-bias-of-a-coin","page":"Getting started","title":"Example: Inferring the bias of a coin","text":"","category":"section"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"The ForneyLab approach to solving inference problems consists of three phases:","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"Model specification: ForneyLab offers a domain-specific language to specify your probabilistic model.\nMessage-passing algorithm generation: Given a model, ForneyLab contructs an algorithm that infers your quantities of interest.\nMessage-passing inference execution: ForneyLab compiles your algorithm to executable (Julia) code to which you may feed data and prior statistics. Executing the algorithm returns (approximate) posterior beliefs for your quantities of inferest.","category":"page"},{"location":"getting-started/#Coin-flip-simulation","page":"Getting started","title":"Coin flip simulation","text":"","category":"section"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"Let's start by gathering some data. One approach could be flipping a coin N times and recording each outcome. Here, however, we will simulate this process by sampling some values from a Bernoulli distribution. Each sample can be thought of as the outcome of single flip which is either heads or tails (1 or 0). We will assume that our virtual coin is biased, and lands heads up on 75% of the trials (on average).","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"N = 25          # number of coin tosses\np = 0.75        # p parameter of the Bernoulli distribution\nsbernoulli(n, p) = [(rand() < p) ? 1 : 0 for _ = 1:n] # define Bernoulli sampler\ndataset = sbernoulli(N, p); # run N Bernoulli trials\nprint(\"dataset = \") ; show(dataset)","category":"page"},{"location":"getting-started/#getting-started-model-specification","page":"Getting started","title":"Model specification","text":"","category":"section"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"In a Bayesian setting, the next step is to specify our probabilistic model. This amounts to specifying the joint probability of the random variables of the system.","category":"page"},{"location":"getting-started/#Likelihood","page":"Getting started","title":"Likelihood","text":"","category":"section"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"We will assume that the outcome of each coin flip is governed by the Bernoulli distribution, i.e.","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"y_i sim Bernoulli(theta),","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"where y_i=1 represents \"heads\", y_i=0 represents \"tails\", and θ in 01 is the underlying probability of the coin landing heads up for a single coin flip.","category":"page"},{"location":"getting-started/#Prior","page":"Getting started","title":"Prior","text":"","category":"section"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"We will choose the conjugate prior of the Bernoulli likelihood function defined above, namely the beta distribution, i.e.","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"theta sim Beta(a b),","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"where a and b are the hyperparameters that encode our prior beliefs about the possible values of theta. We will assign values to the hyperparameters in a later step.   ","category":"page"},{"location":"getting-started/#Joint-probability","page":"Getting started","title":"Joint probability","text":"","category":"section"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"The joint probability is given by the multiplication of the likelihood and the prior, i.e.","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"P(y_i θ) = prod_i=1^N P(y_i  θ) P(θ).","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"Now let's see how to specify this model using ForneyLab's syntax.","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"import ForneyLab: draw\nusing ForneyLab: dot2svg, genDot, FactorGraph, PosteriorFactor\n\nstruct SVG\n    code :: String\nend\nBase.show(io::IO, ::MIME\"image/svg+xml\", b::SVG) = write(io, b.code)\n\ndraw(f::FactorGraph) = SVG(dot2svg(genDot(nodes(f), edges(f))))\nfunction draw(rf::PosteriorFactor)\n    subgraph_nodes = nodes(rf.internal_edges)\n    external_edges = setdiff(edges(subgraph_nodes), rf.internal_edges)\n    SVG(dot2svg(genDot(subgraph_nodes, rf.internal_edges, external_edges=external_edges)))\nend","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"using ForneyLab\ng = FactorGraph()       # create a factor graph\na = placeholder(:a)     # define hyperparameter a as placeholder\nb = placeholder(:b)     # define hyperparameter b as placeholder\n@RV θ ~ Beta(a, b)      # prior\n@RV y ~ Bernoulli(θ)    # likelihood\nplaceholder(y, :y)      # define y as a placeholder for data\ndraw(g)                 # draw the factor graph","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"As you can see, ForneyLab offers a model specification syntax that resembles closely to the mathematical equations defined above. Placeholders are used to indicate variables that take specific values at a later date. For example, the way we feed observations into the model is by iteratively assigning each of the observations in our dataset to the random variable y. Perhaps less obvious is the fact that the hyperparameters a and b are also defined as placeholders. The reason is that we will use them to input our current belief about θ for every observation that is processed. In section Message-passing inference execution we will see how this is done.","category":"page"},{"location":"getting-started/#Message-passing-algorithm-generation","page":"Getting started","title":"Message-passing algorithm generation","text":"","category":"section"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"Once we have defined our model, the next step is to instruct ForneyLab to generate a message-passing algorithm that solves our given inference problem. To do this, we need to specify which type of algorithm we want to use. In this case we will use belief propagation, also known as the sum-product algorithm. Once we execute the following code, we see that a function called step!(...) becomes available in the current scope. This function contains the sum-product message-passing algorithm.","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"# Generate a message passging sum-product algorithm that infers theta\nalgo = messagePassingAlgorithm(θ) # derive a sum-product algorithm to infer θ\nalgo_code = algorithmSourceCode(algo) # convert the algorithm to Julia code\nalgo_expr = Meta.parse(algo_code) # parse the algorithm into a Julia expression\neval(algo_expr) # evaluate the functions contained in the Julia expression\nnothing # hide","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":":(function step!(data::Dict, marginals::Dict=Dict(), messages::Vector{Message}=Array{Message}(undef, 2))\n      #= none:3 =#\n      messages[1] = ruleSPBetaOutNPP(nothing, Message(Univariate, PointMass, m=data[:a]), Message(Univariate, PointMass, m=data[:b]))\n      #= none:4 =#\n      messages[2] = ruleSPBernoulliIn1PN(Message(Univariate, PointMass, m=data[:y]), nothing)\n      #= none:6 =#\n      marginals[:θ] = (messages[1]).dist * (messages[2]).dist\n      #= none:8 =#\n      return marginals\n  end)","category":"page"},{"location":"getting-started/#Message-passing-inference-execution","page":"Getting started","title":"Message-passing inference execution","text":"","category":"section"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"The last step is to execute the message-passing algorithm. In order to do this, we first need to assign values to the hyperparameters a and b which characterize our prior beliefs p(theta) about the bias of the coin. Then, we need to feed the observations, one at a time, to the algorithm together with our current belief (prior) p(theta) about the bias of the coin. The important thing to note here is that the posterior distribution after processing one observation p(thetay_i-1) becomes the prior for the processing of the next observation.","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"# Create a marginals dictionary, and initialize hyperparameters\na = 2.0\nb = 7.0\nmarginals = Dict(:θ => ProbabilityDistribution(Beta, a=a, b=b))\n\nfor i in 1:N\n    # Feed in datapoints 1 at a time\n    data = Dict(:y => dataset[i],\n                :a => marginals[:θ].params[:a],\n                :b => marginals[:θ].params[:b])\n\n    step!(data, marginals)\nend","category":"page"},{"location":"getting-started/#Results","page":"Getting started","title":"Results","text":"","category":"section"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"The plot below shows the result of the inference procedure. We see how the posterior is a “compromise” between the prior and likelihood, as mandated by Bayesian inference.","category":"page"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"using Plots, LaTeXStrings, SpecialFunctions; theme(:default)\nplot(fillalpha=0.3, fillrange = 0, leg=false, xlabel=L\"\\theta\", yticks=nothing)\nBetaPDF(α, β) = x ->  x^(α-1)*(1-x)^(β-1)/beta(α, β) # beta distribution definition\nBernoulliPDF(z, N) = θ -> θ^z*(1-θ)^(N-z) # Bernoulli distribution definition\n\nrθ = range(0, 1, length=100)\np1 = plot(rθ, BetaPDF(a, b), title=\"Prior\", fillalpha=0.3, fillrange = 0, ylabel=L\"P(\\theta)\", c=1,)\np2 = plot(rθ, BernoulliPDF(sum(dataset), N), title=\"Likelihood\", fillalpha=0.3, fillrange = 0, ylabel=L\"P(D|\\theta)\", c=2)\np3 = plot(rθ, BetaPDF(marginals[:θ].params[:a], marginals[:θ].params[:b]), title=\"Posterior\", fillalpha=0.3, fillrange = 0, ylabel=L\"P(\\theta|D)\", c=3)\nplot(p1, p2, p3, layout=@layout([a; b; c]))","category":"page"},{"location":"getting-started/#Where-to-go-next?","page":"Getting started","title":"Where to go next?","text":"","category":"section"},{"location":"getting-started/","page":"Getting started","title":"Getting started","text":"There are a set of demos available in ForneyLab's repository that demonstrate the more advanced features of ForneyLab. Alternatively, you can head to the User guide which provides more detailed information of how to use ForneyLab to solve inference problems.","category":"page"},{"location":"user-guide/#User-guide","page":"User guide","title":"User guide","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"This user guide outlines the usage of ForneyLab for solving inference problems. The main content is divided in three parts:","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Specifying a model\nGenerating an algorithm\nExecuting an algorithm","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"For installation instructions see the Getting started page. To import ForneyLab into the active Julia session run  ","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"using ForneyLab","category":"page"},{"location":"user-guide/#Specifying-a-model","page":"User guide","title":"Specifying a model","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Probabilistic models incorporate the element of randomness to describe an event or phenomenon by using random variables and probability theory. A probabilistic model can be represented diagrammatically by using probabilistic graphical models (PGMs). A factor graph is a type of PGM that is well suited to cast inference tasks in terms of graphical manipulations.","category":"page"},{"location":"user-guide/#Creating-a-new-factor-graph","page":"User guide","title":"Creating a new factor graph","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"In ForneyLab, Factor graphs are represented by a FactorGraph type (struct). To instantiate a new (empty) factor graph we use the constructor without arguments","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"g = FactorGraph()\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"ForneyLab tracks the active factor graph. Operations on the graph, such as adding variables or nodes, only affect the active graph. The call to FactorGraph() creates a new instance of a factor graph and registers it as the active graph.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"To get the active graph run","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"fg = currentGraph()\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"To set the active graph run","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"setCurrentGraph(g)\nnothing # hide","category":"page"},{"location":"user-guide/#Adding-random-variables-(edges)","page":"User guide","title":"Adding random variables (edges)","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Random variables are represented as edges on Forney-style factor graphs. You can add a random variable to the active graph by instantiating a Variable. The constructor accepts an id of type Symbol. For example,","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"x = Variable(id=:x)\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"associates the variable x to an edge of the active graph.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Alternatively, the @RV macro achieves the same by a more compact notation. The following line has the same result as the previous one.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"@RV x\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"By default, the @RV macro uses the Julia-variable's name to create an id for the Variable object (:x in this example). However, if this id was already assigned, then ForneyLab creates a default id of the form :variable_n, where n is a number that increments. In case you want to provide a custom id, the @RV macro accepts an optional keyword argument between square brackets. For example,","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"@RV [id=:my_id] x\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"adds the variable x to the active graph and assigns the :my_id symbol to its id field. Later we will see that this is useful once we start visualizing the factor graph.","category":"page"},{"location":"user-guide/#Adding-factor-nodes","page":"User guide","title":"Adding factor nodes","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Factor nodes are used to define a relationship between random variables. A factor node defines a probability distribution over selected random variables. See Factor nodes for a complete list of the available factor nodes in ForneyLab.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"We assign a probability distribution to a random variable using the ~ operator together with the @RV macro. For example, to create a Gaussian random variable y, where its mean and variance are controlled by the random variables m and v respectively, we define","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"@RV m\n@RV v\n@RV y ~ GaussianMeanVariance(m, v)\nnothing # hide","category":"page"},{"location":"user-guide/#Visualizing-a-factor-graph","page":"User guide","title":"Visualizing a factor graph","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Factor graphs can be visualized using the draw function, which takes a FactorGraph as argument. Let's visualize the factor graph that we defined in the previous section.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"ForneyLab.draw(g)","category":"page"},{"location":"user-guide/#Clamping","page":"User guide","title":"Clamping","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Suppose we know that the variance of the random variable y, is fixed to a certain value. ForneyLab provides a Clamp node to impose such constraints. Clamp nodes can be implicitly defined by using literals like in the following example","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"g = FactorGraph() # create a new factor graph\n@RV m\n@RV y ~ GaussianMeanVariance(m, 1.0)\nForneyLab.draw(g)","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Here, the literal 1.0 creates a clamp node implicitly. Clamp nodes are visualized with a gray background.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Alternatively, if you want to assign a custom id to a Clamp factor node, then you have to instantiate them explicitly using its constructor function, i.e.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"g = FactorGraph() # create a new factor graph\n@RV m\n@RV v ~ Clamp(1.0)\n@RV y ~ GaussianMeanVariance(m, v)\nForneyLab.draw(g)","category":"page"},{"location":"user-guide/#Placeholders","page":"User guide","title":"Placeholders","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Placeholders are Clamp factors that act as entry points for data. They associate a given random variable with a buffer through which data is fed at a later point. This buffer has an id, a dimensionality and a data type. Placeholders are created with the placeholder function. Suppose that we want to feed an array of one-dimensional floating-point data to the y random variable of the previous model. We would then need to define y as a placeholder as follows.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"g = FactorGraph() # create a new factor graph\n@RV m\n@RV v ~ Clamp(1.0)\n@RV y ~ GaussianMeanVariance(m, v)\nplaceholder(y, :y)\nForneyLab.draw(g)","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Placeholders default to one-dimensional floating-point data. In case we want to override this with, for example, 3-dimensional integer data, we would need to specify the dims and datatpye parameters of the placeholder function as follows","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"placeholder(y, :y, dims=(3,), datatype=Int)","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"In the previous example, we first created the random variable y and then marked it as a placeholder. There is, however, a shorthand version to perform these two steps in one. The syntax consists of calling a placeholder method that takes an id Symbol as argument and returns the new random variable. Here is an example:","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"x = placeholder(:x)\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"where x is now a random variable linked to a placeholder with id :x.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"In section Executing an algorithm we will see how the data is fed to the placeholders.","category":"page"},{"location":"user-guide/#Overloaded-operators","page":"User guide","title":"Overloaded operators","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"ForneyLab supports the use of the +, - and * operators between random variables that have certain types of probability distributions. This is known as operator overloading. These operators are represented as deterministic factor nodes in ForneyLab. As an example, a two-component Gaussian mixture can be defined as follows  ","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"g = FactorGraph() # create a new factor graph\n@RV x ~ GaussianMeanVariance(0.0, 1.0)\n@RV y ~ GaussianMeanVariance(2.0, 3.0)\n@RV z = x + y\nplaceholder(z, :z)\nForneyLab.draw(g)","category":"page"},{"location":"user-guide/#Online-vs.-offline-learning","page":"User guide","title":"Online vs. offline learning","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Online learning refers to a procedure where observations are processed as soon as they become available. In the context of factor graphs this means that observations need to be fed to the placeholders and processed one at a time. In a Bayesian setting, this reduces to the application of Bayes rule in a recursive fashion, i.e. the posterior distribution for a given random variable, becomes the prior for the next processing step. Since we are feeding one observation at each time step, the factor graph will have one placeholder for every observed variable. All of the factor graphs that we have seen so far were specified to process data in this fashion.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Let's take a look at an example in order to contrast it with its offline counterpart. In this simple example, the mean x of a Gaussian distributed random variable y is modelled by another Gaussian distribution with hyperparameters m and v. The variance of y is assumed to be known.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"g = FactorGraph() # create a new factor graph\nm = placeholder(:m)\nv = placeholder(:v)\n@RV x ~ GaussianMeanVariance(m, v)\n@RV y ~ GaussianMeanVariance(x, 1.0)\nplaceholder(y, :y)\nForneyLab.draw(g)","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"As we have seen in previous examples, there is one placeholder linked to the observed variable y that accepts one observation at a time. Perhaps what is less obvious is that the hyperparameters m and v are also defined as placeholders. The reason for this is that we will use them to input our current (prior) belief about x for every observation that is processed. In section Executing an algorithm we will elaborate more on this.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Offline learning, on the other hand, involves feeding and processing a batch of N observations (typically all available observations) in a single step. This translates into a factor graph that has one placeholder linked to a random variable for each sample in the batch. We can specify this type of models using a for loop like in the following example.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"g = FactorGraph()   # create a new factor graph\nN = 3               # number of observations\ny = Vector{Variable}(undef, N)\n@RV x ~ GaussianMeanVariance(0.0, 1.0)\nfor i = 1:N\n    @RV y[i] ~ GaussianMeanVariance(x, 1.0)\n    placeholder(y[i], :y, index=i)\nend\nForneyLab.draw(g)","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"The important thing to note here is that we need an extra array of N observed random variables where each of them is linked to a dedicated index of the placeholder's buffer. This buffer can be thought of as an N dimensional array of Clamp factor nodes. We achieve this link by means of the index parameter of the placeholder function.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"In section Executing an algorithm we will see examples of how the data is fed to the placeholders in each of these two scenarios.","category":"page"},{"location":"user-guide/#Generating-an-algorithm","page":"User guide","title":"Generating an algorithm","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"ForneyLab supports code generation for four different types of message-passing algorithms:","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Belief propagation\nVariational message passing\nExpectation maximization\nExpectation propagation","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Whereas belief propagation computes exact inference for the random variables of interest, the variational message passing (VMP), expectation maximization (EM) and expectation propagation (EP) algorithms are approximation methods that can be applied to a larger range of models.","category":"page"},{"location":"user-guide/#Belief-propagation","page":"User guide","title":"Belief propagation","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"The way to instruct ForneyLab to generate a belief propagation algorithm (also known as a sum-product algorithm) is by using the messagePassingAlgorithm function. This function takes as argument(s) the random variable(s) for which we want to infer the posterior distribution. As an example, consider the following hierarchical model in which the mean of a Gaussian distribution is represented by another Gaussian distribution whose mean is modelled by another Gaussian distribution.  ","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"g = FactorGraph() # create a new factor graph\n@RV m2 ~ GaussianMeanVariance(0.0, 1.0)\n@RV m1 ~ GaussianMeanVariance(m2, 1.0)\n@RV y ~ GaussianMeanVariance(m1, 1.0)\nplaceholder(y, :y)\nForneyLab.draw(g)","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"If we were only interested in inferring the posterior distribution of m1 then we would run","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"algorithm = messagePassingAlgorithm(m1)\nalgorithm_code = algorithmSourceCode(algorithm)\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"On the other hand, if we were interested in the posterior distributions of both m1 and m2 we would then need to pass them as elements of an array, i.e.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"algorithm = messagePassingAlgorithm([m1, m2])\nalgorithm_code = algorithmSourceCode(algorithm)\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Note that the message-passing algorithm returned by the algorithmSourceCode function is a String that contains the definition of a Julia function. In order to access this function, we need to parse the code and evaluate it in the current scope. This can be done as follows","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"algorithm_expr = Meta.parse(algorithm_code)\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":":(function step!(data::Dict, marginals::Dict=Dict(), messages::Vector{Message}=Array{Message}(undef, 4))\n      #= none:3 =#\n      messages[1] = ruleSPGaussianMeanVarianceOutNPP(nothing, Message(Univariate, PointMass, m=0.0), Message(Univariate, PointMass, m=1.0))\n      #= none:4 =#\n      messages[2] = ruleSPGaussianMeanVarianceOutNGP(nothing, messages[1], Message(Univariate, PointMass, m=1.0))\n      #= none:5 =#\n      messages[3] = ruleSPGaussianMeanVarianceMPNP(Message(Univariate, PointMass, m=data[:y]), nothing, Message(Univariate, PointMass, m=1.0))\n      #= none:6 =#\n      messages[4] = ruleSPGaussianMeanVarianceMGNP(messages[3], nothing, Message(Univariate, PointMass, m=1.0))\n      #= none:8 =#\n      marginals[:m1] = (messages[2]).dist * (messages[3]).dist\n      #= none:9 =#\n      marginals[:m2] = (messages[1]).dist * (messages[4]).dist\n      #= none:11 =#\n      return marginals\n  end)","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"eval(algorithm_expr)\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"At this point a new function named step! becomes available in the current scope. This function contains a message-passing algorithm that infers both m1 and m2 given one or more y observations. In the section Executing an algorithm we will see how this function is used.","category":"page"},{"location":"user-guide/#Variational-message-passing","page":"User guide","title":"Variational message passing","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Variational message passing (VMP) algorithms are generated much in the same way as the belief propagation algorithm we saw in the previous section. There is a major difference though: for VMP algorithm generation we need to define the factorization properties of our approximate distribution. A common approach is to assume that all random variables of the model factorize with respect to each other. This is known as the mean field assumption. In ForneyLab, the specification of such factorization properties is defined using the PosteriorFactorization composite type. Let's take a look at a simple example to see how it is used. In this model we want to learn the mean and variance of a Gaussian distribution, where the former is modelled with a Gaussian distribution and the latter with a Gamma.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"g = FactorGraph() # create a new factor graph\n@RV m ~ GaussianMeanVariance(0, 10)\n@RV w ~ Gamma(0.1, 0.1)\n@RV y ~ GaussianMeanPrecision(m, w)\nplaceholder(y, :y)\ndraw(g)","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"The construct of the PosteriorFactorization composite type takes the random variables of interest as arguments and one final argument consisting of an array of symbols used to identify each of these random variables. Here is an example of how to use this construct for the previous model where we want to infer m and w.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"q = PosteriorFactorization(m, w, ids=[:M, :W])\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Here, the PosteriorFactorization constructor specifies a posterior factorization. We can view the posterior factorization as dividing the factor graph into several subgraphs, each corresponding to a separate factor in the posterior factorization. Minimization of the free energy is performed by iterating over each subgraph in order to update the posterior marginal corresponding to the current factor which depends on messages coming from the other subgraphs. This iteration is repeated until either the free energy converges to a certain value or the posterior marginals of each factor stop changing. We can use the ids passed to the PosteriorFactorization constructor to visualize the corresponding subgraphs, as shown below.   ","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"ForneyLab.draw(q.posterior_factors[:M])","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"ForneyLab.draw(q.posterior_factors[:W])","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Generating the VMP algorithm then follows a similar same procedure as the belief propagation algorithm. In the VMP case however, the resulting algorithm will consist of multiple step functions, one for each posterior factor, that need to be executed iteratively until convergence.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"# Construct and compile a variational message passing algorithm\nalgo = messagePassingAlgorithm()\nalgo_code = algorithmSourceCode(algo)\neval(Meta.parse(algo_code))\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Meta.parse(algo) = quote\n    #= none:3 =#\n    function stepM!(data::Dict, marginals::Dict=Dict(), messages::Vector{Message}=Array{Message}(undef, 2))\n        #= none:5 =#\n        messages[1] = ruleVBGaussianMeanVarianceOut(nothing, ProbabilityDistribution(Univariate, PointMass, m=0), ProbabilityDistribution(Univariate, PointMass, m=10))\n        #= none:6 =#\n        messages[2] = ruleVBGaussianMeanPrecisionM(ProbabilityDistribution(Univariate, PointMass, m=data[:y]), nothing, marginals[:w])\n        #= none:8 =#\n        marginals[:m] = (messages[1]).dist * (messages[2]).dist\n        #= none:10 =#\n        return marginals\n    end\n    #= none:14 =#\n    function stepW!(data::Dict, marginals::Dict=Dict(), messages::Vector{Message}=Array{Message}(undef, 2))\n        #= none:16 =#\n        messages[1] = ruleVBGammaOut(nothing, ProbabilityDistribution(Univariate, PointMass, m=0.1), ProbabilityDistribution(Univariate, PointMass, m=0.1))\n        #= none:17 =#\n        messages[2] = ruleVBGaussianMeanPrecisionW(ProbabilityDistribution(Univariate, PointMass, m=data[:y]), marginals[:m], nothing)\n        #= none:19 =#\n        marginals[:w] = (messages[1]).dist * (messages[2]).dist\n        #= none:21 =#\n        return marginals\n    end\nend","category":"page"},{"location":"user-guide/#Computing-free-energy","page":"User guide","title":"Computing free energy","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"VMP inference boils down to finding the member of a family of tractable probability distributions that is closest in KL divergence to an intractable posterior distribution. This is achieved by minimizing a quantity known as free energy. ForneyLab offers to optionally compile code for evaluating the free energy. Free energy is particularly useful to test for convergence of the VMP iterative procedure.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"algo = messagePassingAlgorithm(free_energy=true)\nalgo_code = algorithmSourceCode(algo, free_energy=true)\neval(Meta.parse(algo_code))","category":"page"},{"location":"user-guide/#Expectation-maximization","page":"User guide","title":"Expectation maximization","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"The expectation step of the expectation maximization (EM) algorithm can be executed by sum-product message passing, while the maximization step can be performed analytically or numerically. ","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"g = FactorGraph()\nv = placeholder(:v) # parameter of interest\n@RV m ~ GaussianMeanVariance(0.0, 1.0)\n@RV y ~ GaussianMeanVariance(m, v)\nplaceholder(y, :y)\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Generating a sum-product algorithm towards a clamped variable signals ForneyLab that the user wishes to optimize this parameter. Next to returning a step!() function, ForneyLab will then provide mockup code for an optimize!() function as well.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"algo = messagePassingAlgorithm(v)\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"# You have created an algorithm that requires updates for (a) clamped parameter(s).\n# This algorithm requires the definition of a custom `optimize!` function that updates the parameter value(s)\n# by altering the `data` dictionary in-place. The custom `optimize!` function may be based on the mockup below:\n\n# function optimize!(data::Dict, marginals::Dict=Dict(), messages::Vector{Message}=init())\n#   ...\n#   return data\n# end","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"This optimize!() function accepts This function needs to be defined by the user and may optionally depend upon external tools. Note that optimize!() and step!() functions have the same calling signature. However, where a step!() function accepts the data and updates the messages and marginals, the optimize!() function operates vice-versa. Iterating these functions offers a general mechanism for implementing EM-type algorithms. See the demos for a detailed example.","category":"page"},{"location":"user-guide/#Executing-an-algorithm","page":"User guide","title":"Executing an algorithm","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"In section Specifying a model we introduced two main ways to learn from data, namely in an online and in an offline setting. We saw that the structure of the factor graph is different in each of these settings. In this section we will demonstrate how to feed data to an algorithm in both an online and an offline setting. We will use the same examples from section Online vs. offline learning.","category":"page"},{"location":"user-guide/#Online-learning","page":"User guide","title":"Online learning","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"For convenience, let's reproduce the model specification for the problem of estimating the mean x of a Gaussian distributed random variable y, where x is modelled using another Gaussian distribution with hyperparameters m and v. Let's also generate a belief propagation algorithm for this inference problem like we have seen before.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"using ForneyLab #hide\n\ng = FactorGraph() # create a new factor graph\npfz = PosteriorFactorization() # hide\n\nm = placeholder(:m)\nv = placeholder(:v)\n@RV x ~ GaussianMeanVariance(m, v)\n@RV y ~ GaussianMeanVariance(x, 1.0)\nplaceholder(y, :y)\n\nalgo = messagePassingAlgorithm(x)\neval(Meta.parse(algorithmSourceCode(algo))) # generate, parse and evaluate the algorithm\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"In order to execute this algorithm we first have to specify a prior for x. This is done by choosing some initial values for the hyperparameters m and v. In each processing step, the algorithm expects an observation and the current belief about x, i.e. the prior. We pass this information as elements of a data dictionary where the keys are the ids of their corresponding placeholders. The algorithm performs inference and returns the results inside a different dictionary (which we call marginals in the following script). In the next iteration, we repeat this process by feeding the algorithm with the next observation in the sequence and the posterior distribution of x that we obtained in the previous processing step. In other words, the current posterior becomes the prior for the next processing step. Let's illustrate this using an example where we will first generate a synthetic dataset by sampling observations from a Gaussian distribution that has a mean of 5.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"using Plots, LaTeXStrings; theme(:default) ;\nplot(fillalpha=0.3, leg=false, xlabel=L\"x\", ylabel=L\"p(x|D)\", yticks=nothing)\n\nN = 50                      # number of samples\ndataset = randn(N) .+ 5     # sample N observations from a Gaussian with m=5 and v=1\n\nnormal(μ, σ²) = x -> (1/(sqrt(2π*σ²))) * exp(-(x - μ)^2 / (2*σ²)) # to plot results\n\nm_prior = 0.0   # initialize hyperparameter m\nv_prior = 10    # initialize hyperparameter v\n\nmarginals = Dict()  # this is where the algorithm stores the results\n\nanim = @animate for i in 1:N\n    data = Dict(:y => dataset[i],\n                :m => m_prior,\n                :v => v_prior)\n\n    plot(-10:0.01:10, normal(m_prior, v_prior), fill=true)\n\n    step!(data, marginals) # feed in prior and data points 1 at a time\n\n    global m_prior = mean(marginals[:x]) # today's posterior is tomorrow's prior\n    global v_prior = var(marginals[:x])  # today's posterior is tomorrow's prior\nend\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"(Image: Online learning)","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"As we process more samples, our belief about the possible values of m becomes more confident.","category":"page"},{"location":"user-guide/#Offline-learning","page":"User guide","title":"Offline learning","text":"","category":"section"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Executing an algorithm in an offline fashion is much simpler than in the online case. Let's reproduce the model specification of the previous example in an offline setting (also shown in Online vs. offline learning.)","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"g = FactorGraph()   # create a new factor graph\npfz = PosteriorFactorization() # hide\nN = 30              # number of observations\ny = Vector{Variable}(undef, N)\n@RV x ~ GaussianMeanVariance(0.0, 1.0)\nfor i = 1:N\n    @RV y[i] ~ GaussianMeanVariance(x, 1.0)\n    placeholder(y[i], :y, index=i)\nend\n\nalgo = messagePassingAlgorithm(x)\neval(Meta.parse(algorithmSourceCode(algo))) # generate, parse and evaluate the algorithm\nnothing # hide","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"Since we have a placeholder linked to each observation in the sequence, we can process the complete dataset in one step. To do so, we first need to create a dictionary having the complete dataset array as its single element. We then need to pass this dictionary to the step! function which, in contrast with the online counterpart, we only need to call once.","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"data = Dict(:y => dataset)\nmarginals = step!(data) # Run the algorithm\nplot(-10:0.01:10, normal(mean(marginals[:x]), var(marginals[:x])), fillalpha=0.3, fillrange = 0)","category":"page"},{"location":"user-guide/","page":"User guide","title":"User guide","text":"note: Note\nForneyLab is aimed at processing time-series data; batch processing does not perform well with large datasets at the moment. We are working on this issue.","category":"page"},{"location":"contributing/#Contribution-guidelines","page":"Contributing","title":"Contribution guidelines","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"This page details the some of the guidelines that should be followed when contributing to this package.","category":"page"},{"location":"contributing/#Reporting-bugs","page":"Contributing","title":"Reporting bugs","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We track bugs using GitHub issues. We encourage you to write complete, specific, reproducible bug reports. Mention the versions of Julia and ForneyLab for which you observe unexpected behavior. Please provide a concise description of the problem and complement it with code snippets, test cases, screenshots, tracebacks or any other information that you consider relevant. This will help us to replicate the problem and narrow the search space for solutions.","category":"page"},{"location":"contributing/#Suggesting-features","page":"Contributing","title":"Suggesting features","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We welcome new feature proposals. However, before submitting a feature request, consider a few things:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Does the feature require changes in the core ForneyLab.jl code? If it doesn't (for example, you would like to add a node for a particular application), consider making a separate repository for your extensions.\nIf you would like to add an implementation of a feature that changes a lot in the core ForneyLab.jl code, please open an issue on GitHub and describe your proposal first. This enables us to discuss your proposal before you invest your time in implementing something that may be difficult to merge later on.","category":"page"},{"location":"contributing/#Contributing-code","page":"Contributing","title":"Contributing code","text":"","category":"section"},{"location":"contributing/#Installing-ForneyLab","page":"Contributing","title":"Installing ForneyLab","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We suggest that you use the dev command from the new Julia package manager to install ForneyLab.jl for development purposes. To work on your fork of ForneyLab.jl, use your fork's URL address in the dev command, for example:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"] dev git@github.com:your_username/ForneyLab.jl.git","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"The dev command clones ForneyLab.jl to ~/.julia/dev/ForneyLab. All local changes to ForneyLab code will be reflected in imported code.","category":"page"},{"location":"contributing/#Committing-code","page":"Contributing","title":"Committing code","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We use the standard GitHub Flow workflow where all contributions are added through pull requests. In order to contribute, first fork the repository, then commit your contributions to your fork, and then create a pull request on the master branch of the ForneyLab.jl repository.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Before opening a pull request, please make sure that all tests pass. All demos (can be found in /demo/ directory) have to run without error as well.","category":"page"},{"location":"contributing/#Style-conventions","page":"Contributing","title":"Style conventions","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We use default Julia style guide. We list here a few important points and our modifications to the Julia style guide:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Use 4 spaces for indentation\nType names use UpperCamelCase. E.g., FactorNode, Gaussian\nFunction names are lowerCamelCase (differs from the official Julia convention). For example, isApplicable, currentGraph\nVariable names and function arguments (e.g. inbound_messages) use snake_case\nThe name of a method that modifies its argument(s) must end in !","category":"page"},{"location":"contributing/#Unit-tests","page":"Contributing","title":"Unit tests","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We use the test-driven development (TDD) methodology for ForneyLab.jl development. The test coverage should be as complete as possible. Please make sure that you write tests for each piece of code that you want to add.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"All unit tests are located in the /test/ directory. The /test/ directory follows the structure of the /src/ directory. Each test file should have following filename format: test_*.jl.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"The tests can be evaluated by running following command in the Julia REPL:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"] test ForneyLab","category":"page"},{"location":"library/developer-api/#Developer-API","page":"Developer API","title":"Developer API","text":"","category":"section"},{"location":"library/developer-api/","page":"Developer API","title":"Developer API","text":"Documentation for ForneyLab.jl's developer API.","category":"page"},{"location":"library/developer-api/#Contents","page":"Developer API","title":"Contents","text":"","category":"section"},{"location":"library/developer-api/","page":"Developer API","title":"Developer API","text":"Pages = [\"developer-api.md\"]\nDepth = 5","category":"page"},{"location":"library/developer-api/#Index","page":"Developer API","title":"Index","text":"","category":"section"},{"location":"library/developer-api/","page":"Developer API","title":"Developer API","text":"Modules = [ForneyLab]\nPages = [\"developer-api.md\"]\nOrder = [:macro, :module, :constant, :type, :function]","category":"page"},{"location":"library/developer-api/#Extended-rules-registration","page":"Developer API","title":"Extended rules registration","text":"","category":"section"},{"location":"library/developer-api/","page":"Developer API","title":"Developer API","text":"ForneyLab.@expectationPropagationRule\nForneyLab.@marginalRule\nForneyLab.@naiveVariationalRule\nForneyLab.@structuredVariationalRule\nForneyLab.@sumProductRule","category":"page"},{"location":"library/developer-api/#ForneyLab.@expectationPropagationRule","page":"Developer API","title":"ForneyLab.@expectationPropagationRule","text":"@expectationPropagationRule registers a expectation propagation update rule by defining the rule type and the corresponding methods for the outboundType and isApplicable functions. If no name (type) for the new rule is passed, a unique name (type) will be generated. Returns the rule type.\n\n\n\n\n\n","category":"macro"},{"location":"library/developer-api/#ForneyLab.@marginalRule","page":"Developer API","title":"ForneyLab.@marginalRule","text":"@marginalRule registers a marginal update rule for a (joint) marginal by defining the rule type and the corresponding methods for the isApplicable function. If no name (type) for the new rule is passed, a unique name (type) will be generated. Returns the rule type.\n\n\n\n\n\n","category":"macro"},{"location":"library/developer-api/#ForneyLab.@naiveVariationalRule","page":"Developer API","title":"ForneyLab.@naiveVariationalRule","text":"@naiveVariationalRule registers a variational update rule for the naive (mean-field) factorization by defining the rule type and the corresponding methods for the outboundType and isApplicable functions. If no name (type) for the new rule is passed, a unique name (type) will be generated. Returns the rule type.\n\n\n\n\n\n","category":"macro"},{"location":"library/developer-api/#ForneyLab.@structuredVariationalRule","page":"Developer API","title":"ForneyLab.@structuredVariationalRule","text":"@structuredVariationalRule registers a variational update rule for the structured factorization by defining the rule type and the corresponding methods for the outboundType and isApplicable functions. If no name (type) for the new rule is passed, a unique name (type) will be generated. Returns the rule type.\n\n\n\n\n\n","category":"macro"},{"location":"library/developer-api/#ForneyLab.@sumProductRule","page":"Developer API","title":"ForneyLab.@sumProductRule","text":"@sumProductRule registers a sum-product update rule by defining the rule type and the corresponding methods for the outboundType and isApplicable functions. If no name (type) for the new rule is passed, a unique name (type) will be generated. Returns the rule type.\n\n\n\n\n\n","category":"macro"},{"location":"library/developer-api/#Graph-(low-level)","page":"Developer API","title":"Graph (low-level)","text":"","category":"section"},{"location":"library/developer-api/","page":"Developer API","title":"Developer API","text":"ForneyLab.Edge\nForneyLab.Interface\nForneyLab.Terminal","category":"page"},{"location":"library/developer-api/#ForneyLab.Edge","page":"Developer API","title":"ForneyLab.Edge","text":"An Edge joins two interfaces (half-edges) a and b.\n\n\n\n\n\n","category":"type"},{"location":"library/developer-api/#ForneyLab.Interface","page":"Developer API","title":"ForneyLab.Interface","text":"An Interface belongs to a FactorNode and represents a half-edge. An Interface has at most one partner interface, with wich it forms an edge.\n\n\n\n\n\n","category":"type"},{"location":"library/developer-api/#ForneyLab.Terminal","page":"Developer API","title":"ForneyLab.Terminal","text":"Description:\n\nTerminal is a special type of node that is only used in the internal\ngraph of a CompositeFactor. A Terminal is used to terminate an Edge in the\ninternal graph that is linked to an interface of the CompositeFactor.\n\nA Terminal is linked to an interface of the\nCompositeFactor containing the Terminal.\n\nInterfaces:\n\n1. out\n\nConstruction:\n\nTerminal(id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/developer-api/#Scheduler-(low-level)","page":"Developer API","title":"Scheduler (low-level)","text":"","category":"section"},{"location":"library/developer-api/","page":"Developer API","title":"Developer API","text":"ForneyLab.MarginalRule\nForneyLab.MarginalEntry\nForneyLab.MarginalUpdateRule\nForneyLab.MessageUpdateRule\nForneyLab.PosteriorFactor\nForneyLab.ScheduleEntry","category":"page"},{"location":"library/developer-api/#ForneyLab.MarginalRule","page":"Developer API","title":"ForneyLab.MarginalRule","text":"MarginalRule{factor_type} specifies a joint marginal update rule with respect to a node of type factor_type.\n\n\n\n\n\n","category":"type"},{"location":"library/developer-api/#ForneyLab.MarginalEntry","page":"Developer API","title":"ForneyLab.MarginalEntry","text":"A MarginalEntry defines a marginal computation. The marginal_update_rule <: MarginalUpdateRule defines the rule that is used to calculate the (joint) marginal over target.\n\n\n\n\n\n","category":"type"},{"location":"library/developer-api/#ForneyLab.MarginalUpdateRule","page":"Developer API","title":"ForneyLab.MarginalUpdateRule","text":"A MarginalUpdateRule specifies how a (joint) marginal is calculated from incoming messages (and a node function).\n\n\n\n\n\n","category":"type"},{"location":"library/developer-api/#ForneyLab.MessageUpdateRule","page":"Developer API","title":"ForneyLab.MessageUpdateRule","text":"A MessageUpdateRule specifies how a Message is calculated from the node function and the incoming messages. Use subtypes(MessageUpdateRule) to list the available rules.\n\n\n\n\n\n","category":"type"},{"location":"library/developer-api/#ForneyLab.PosteriorFactor","page":"Developer API","title":"ForneyLab.PosteriorFactor","text":"A PosteriorFactor specifies the subset of variables that comprise a joint factor in the posterior factorization. A PosteriorFactor can be defined by a (selection of) variable(s), or on the complete graph.\n\n\n\n\n\n","category":"type"},{"location":"library/developer-api/#ForneyLab.ScheduleEntry","page":"Developer API","title":"ForneyLab.ScheduleEntry","text":"A ScheduleEntry defines a message computation. The message_update_rule <: MessageUpdateRule defines the rule that is used to calculate the message coming out of interface.\n\n\n\n\n\n","category":"type"},{"location":"#ForneyLab.jl","page":"Home","title":"ForneyLab.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia toolbox for automatic generation of (Bayesian) inference algorithms.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Given a probabilistic model, ForneyLab generates efficient Julia code for message-passing based inference. It uses the model structure to generate an algorithm that consists of a sequence of local computations on a Forney-style factor graph (FFG) representation of the model.","category":"page"},{"location":"#Package-Features","page":"Home","title":"Package Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"User friendly syntax for specification of probabilistic models.\nAutomatic generation of message passing algorithms including\nBelief propagation\nVariational message passing\nExpectation maximization\nExpectation propagation\nSupport for hybrid models combining discrete and continuous latent variables.\nEvaluation of free energy as a model performance measure.\nCombination of distinct inference algorithms under a unified paradigm.\nFeatures composite nodes that allow for flexible hierarchical design in terms of model structure and algorithms.","category":"page"},{"location":"#Resources","page":"Home","title":"Resources","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For an in-depth overview of ForneyLab, see A Factor Graph Approach to Automated Design of Bayesian Signal Processing Algorithms by Cox et. al. (2018).\nFor an introduction to message passing and FFGs, see The Factor Graph Approach to Model-Based Signal Processing by Loeliger et al. (2007).\nThe ForneyLab project page provides more background on ForneyLab as well as pointers to related literature and talks.","category":"page"},{"location":"#How-to-get-started?","page":"Home","title":"How to get started?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Head to the Getting started section to get up and running with ForneyLab.","category":"page"},{"location":"internals/#Internals","page":"Internals","title":"Internals","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"This page summarizes some of ForneyLab's internal structures. It is mainly tailored for software developers interested in a high-level overview of the inner workings of the package. Coding style conventions can be found in STYLEGUIDE.md.","category":"page"},{"location":"internals/#Directory-structure","page":"Internals","title":"Directory structure","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"ForneyLab's directories and files are structured as follows:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"/demo/: demos in Jupyter (iJulia) notebook format (.ipynb)\n/docs/: documentation source and build location\n/src/: ForneyLab source files\nalgorithms/: inference algorithm implementations\nengines/: rendering of message passing schedules to executable code\njulia/: Julia engine and update rule implementations\nfactor_nodes/: all node-specific files\nupdate_rules/: message passing update rules\n/test/: test files with directory structure similar to /src/.","category":"page"},{"location":"internals/#InferenceAlgorithm-data-structure","page":"Internals","title":"InferenceAlgorithm data structure","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"A ForneyLab InferenceAlgorithm is structured as follows:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"algorithm::InferenceAlgorithm: specifies everything required for algorithm generation\nposterior_factorization::PosteriorFactorization: specifies factorization of the posterior\nposterior_factors::Vector{PosteriorFactor} (per item):\nid::Symbol: posterior factor id\noptimize::Bool: require optimization block\ninitialize::Bool: require initialization block\nschedule::Schedule (per ScheduleEntry item):\nschedule_index::Int: position of entry in schedule\nmessage_update_rule::Type: update rule type for message computation\ninitialize::Bool: require message initialization\nfamily::FactorFunction: family of message distribution (for initialization)\ndimensionality::Tuple: dimensionality of message distribution (for initialization)\ninbounds::Vector (per item):\ninbound::Union: inbound, see below\nmarginal_table::MarginalTable (per MarginalEntry item):\nmarginal_id::Symbol: identifier for the marginal\nmarginal_update_rule::Union{Nothing, Product, Type}: update rule type for marginal computation\ninbounds::Vector (per item):\ninbound::Union: inbound, see below\naverage_energies::Dict (per item):\nnode::Type: node type for average energy computation\ncounting_number::Int64: counting number for average energy\ninbounds::Vector (per item):\ninbound::Union: inbound, see below\nentropies::Dict (per item):\ncounting_number::Int64: counting number for (joint) entropy\ninbound::Union: inbound, see below","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"Inbounds are of type Union{Nothing, ScheduleEntry, MarginalEntry, Dict, Clamp}.","category":"page"},{"location":"internals/#Update-rules-naming-convention","page":"Internals","title":"Update rules naming convention","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"The name of an update rule is composed of several parts:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"The word rule\nType of algorithm\nSP: sum-product\nVB: variational Bayes\nSVB: structured variational Bayes\nM: marginal (joint)\nType of factor node\nInterface of the outgoing message\nTypes of incoming messages (absent for VB rules)\nN: Nothing\nP: point mass\nD: distribution\n[I]: first letter of the message's probability distribution","category":"page"},{"location":"internals/#Example-1:-ruleSPGaussianMeanPrecisionMPNP","page":"Internals","title":"Example 1: ruleSPGaussianMeanPrecisionMPNP","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"rule : update rule\nSP : sum-product algorithm\nGaussianMeanPrecision: Gaussian mean precision factor node\nM: outgoing message through 'Mean' interface\nPNP: incoming message types are: point mass, Nothing and point mass","category":"page"},{"location":"internals/#Example-2:-ruleVBBernoulliOut","page":"Internals","title":"Example 2: ruleVBBernoulliOut","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"rule: update rule\nVB: variational Bayes algorithm\nBernoulli: Bernoulli factor node\nOut: outgoing message through 'Out' interface\n-","category":"page"},{"location":"internals/#Example-3:-ruleEPProbitIn1BG","page":"Internals","title":"Example 3: ruleEPProbitIn1BG","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"rule: update rule\nEP: expectation propagation algorithm\nProbit: probit factor node\nIn1: outgoing message through 'in1' interface\nGB: incoming message types are: Gaussian and Bernoulli","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"Note that EP update rules do not have N (nothing) in the set of incoming messages given that in EP there is an incoming message through the interface of the outgoing message that is being calculated.","category":"page"},{"location":"library/user-api/#User-API","page":"User API","title":"User API","text":"","category":"section"},{"location":"library/user-api/","page":"User API","title":"User API","text":"Documentation for ForneyLab.jl's user API.","category":"page"},{"location":"library/user-api/","page":"User API","title":"User API","text":"If you want to know how you can extend ForneyLab.jl (e.g. register new update rules), see Developer API.","category":"page"},{"location":"library/user-api/#Contents","page":"User API","title":"Contents","text":"","category":"section"},{"location":"library/user-api/","page":"User API","title":"User API","text":"Pages = [\"user-api.md\"]\nDepth = 5","category":"page"},{"location":"library/user-api/#Index","page":"User API","title":"Index","text":"","category":"section"},{"location":"library/user-api/","page":"User API","title":"User API","text":"Modules = [ForneyLab]\nPages = [\"user-api.md\"]\nOrder = [:macro, :module, :constant, :type, :function]","category":"page"},{"location":"library/user-api/#Model-specification","page":"User API","title":"Model specification","text":"","category":"section"},{"location":"library/user-api/","page":"User API","title":"User API","text":"ForneyLab.@RV\nForneyLab.FactorGraph\nForneyLab.Variable\nForneyLab.currentGraph","category":"page"},{"location":"library/user-api/#ForneyLab.@RV","page":"User API","title":"ForneyLab.@RV","text":"@RV provides a convenient way to add Variables and FactorNodes to the graph.\n\nExamples:\n\n# Automatically create new Variable x, try to assign x.id = :x if this id is available\n@RV x ~ GaussianMeanVariance(constant(0.0), constant(1.0))\n\n# Explicitly specify the id of the Variable\n@RV [id=:my_y] y ~ GaussianMeanVariance(constant(0.0), constant(1.0))\n\n# Automatically assign z.id = :z if this id is not yet taken\n@RV z = x + y\n\n# Manual assignment\n@RV [id=:my_u] u = x + y\n\n# Just create a variable\n@RV x\n@RV [id=:my_x] x\n\n\n\n\n\n","category":"macro"},{"location":"library/user-api/#ForneyLab.FactorGraph","page":"User API","title":"ForneyLab.FactorGraph","text":"A factor graph consisting of factor nodes and edges.\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Variable","page":"User API","title":"ForneyLab.Variable","text":"A Variable encompasses one or more edges in a FactorGraph.\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.currentGraph","page":"User API","title":"ForneyLab.currentGraph","text":"Return currently active FactorGraph. Create one if there is none.\n\n\n\n\n\n","category":"function"},{"location":"library/user-api/#Factor-nodes","page":"User API","title":"Factor nodes","text":"","category":"section"},{"location":"library/user-api/","page":"User API","title":"User API","text":"Modules = [ForneyLab]\nPrivate = false\nPages = collect(Iterators.flatten([[joinpath(root[4:end], file) for file in files] for (root, dirs, files) in walkdir(\"../src/factor_nodes/\")]))\nOrder = [:macro, :module, :constant, :type, :function]","category":"page"},{"location":"library/user-api/#ForneyLab.Addition","page":"User API","title":"ForneyLab.Addition","text":"Description:\n\nAn addition constraint factor node.\n\nf(out,in1,in2) = δ(in1 + in2 - out)\n\nInterfaces:\n\n1. out\n2. in1\n3. in2\n\nConstruction:\n\nAddition(out, in1, in2, id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#Base.:--Tuple{Variable, Variable}","page":"User API","title":"Base.:-","text":"-(in1::Variable, in2::Variable)\n\nA subtraction constraint based on the addition factor node.\n\n\n\n\n\n","category":"method"},{"location":"library/user-api/#ForneyLab.Bernoulli","page":"User API","title":"ForneyLab.Bernoulli","text":"Description:\n\nBernoulli factor node\n\nout ∈ {0, 1}\np ∈ [0, 1]\n\nf(out, p) = Ber(out|p) = p^out (1 - p)^{1 - out}\n\nInterfaces:\n\n1. out\n2. p\n\nConstruction:\n\nBernoulli(id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Beta","page":"User API","title":"ForneyLab.Beta","text":"Description:\n\nBeta factor node\n\nReal scalars\na > 0\nb > 0\n\nf(out, a, b) = Beta(out|a, b) = Γ(a + b)/(Γ(a) Γ(b)) out^{a - 1} (1 - out)^{b - 1}\n\nInterfaces:\n\n1. out\n2. a\n3. b\n\nConstruction:\n\nBeta(id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Categorical","page":"User API","title":"ForneyLab.Categorical","text":"Description:\n\nCategorical factor node\n\nThe categorical node defines a one-dimensional probability\ndistribution over the normal basis vectors of dimension d\n\nout ∈ {0, 1}^d where Σ_k out_k = 1\np ∈ [0, 1]^d, where Σ_k p_k = 1\n\nf(out, p) = Cat(out | p)\n          = Π_i p_i^{out_i}\n\nInterfaces:\n\n1. out\n2. p\n\nConstruction:\n\nCategorical(id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.ChanceConstraint","page":"User API","title":"ForneyLab.ChanceConstraint","text":"Description:\n\nChance constraint on the marginal q of the connected variable x, as\nϵ ⩽ 1 - ∫_G q(x) dx, where G indicates a region. In other words,\nthe probability mass of q is not allowed to overflow the region G\nby more than ϵ. Implementation according to (van de Laar et al.\n\"Chance-Constrained Active Inference\", MIT Neural Computation, 2021).\n\nInterfaces:\n\n1. out\n\nConstruction:\n\nChanceConstraint(out; G=(min,max), epsilon=epsilon, id=:my_node)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.@ensureVariables-Tuple","page":"User API","title":"ForneyLab.@ensureVariables","text":"@ensureVariables(...) casts all non-Variable arguments to Variable through constant(arg).\n\n\n\n\n\n","category":"macro"},{"location":"library/user-api/#ForneyLab.Clamp","page":"User API","title":"ForneyLab.Clamp","text":"Description:\n\nA factor that clamps a variable to a constant value.\n\nf(out) = δ(out - value)\n\nInterfaces:\n\n1. out\n\nConstruction:\n\nClamp(out, value, id=:some_id)\nClamp(value, id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.constant-Tuple{Any}","page":"User API","title":"ForneyLab.constant","text":"constant creates a Variable which is linked to a new Clamp, and returns this variable.\n\ny = constant(3.0, id=:y)\n\n\n\n\n\n","category":"method"},{"location":"library/user-api/#ForneyLab.placeholder-Tuple{Variable, Symbol}","page":"User API","title":"ForneyLab.placeholder","text":"placeholder(...) creates a Clamp node and registers this node as a data placeholder with the current graph.\n\n# Link variable y to buffer with id :y,\n# indicate that Clamp will hold Float64 values.\nplaceholder(y, :y, datatype=Float64)\n\n# Link variable y to index 3 of buffer with id :y.\n# Specify the data type by passing a default value for the Clamp.\nplaceholder(y, :y, index=3, default=0.0)\n\n# Indicate that the Clamp will hold an array of size `dims`,\n# with Float64 elements.\nplaceholder(X, :X, datatype=Float64, dims=(3,2))\n\n\n\n\n\n","category":"method"},{"location":"library/user-api/#ForneyLab.@composite-Tuple{Symbol, Expr, Expr}","page":"User API","title":"ForneyLab.@composite","text":"The @composite macro allows for defining custom (composite) nodes. Composite nodes allow for implementating of custom update rules that may be computationally more efficient or convenient. A composite node can be defined with or without an  internal model. For detailed usage instructions we refer to the composite_nodes demo.\n\n\n\n\n\n","category":"macro"},{"location":"library/user-api/#ForneyLab.Contingency","page":"User API","title":"ForneyLab.Contingency","text":"Description:\n\nContingency factor node\n\nThe contingency distribution is a multivariate generalization of\nthe categorical distribution. As a bivariate distribution, the\ncontingency distribution defines the joint probability\nover two unit vectors. The parameter p encodes a contingency matrix\nthat specifies the probability of co-occurrence.\n\nout1 ∈ {0, 1}^d1 where Σ_j out1_j = 1\nout2 ∈ {0, 1}^d2 where Σ_k out2_k = 1\np ∈ [0, 1]^{d1 × d2}, where Σ_jk p_jk = 1\n\nf(out1, out2, p) = Con(out1, out2 | p)\n                 = Π_jk p_jk^{out1_j * out2_k}\n\nA Contingency distribution over more than two variables requires\nhigher-order tensors as parameters; these are not implemented in ForneyLab.\n\nInterfaces:\n\n1. out1\n2. out2\n3. p\n\nConstruction:\n\nContingency(id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Dirichlet","page":"User API","title":"ForneyLab.Dirichlet","text":"Description:\n\nDirichlet factor node\n\nMultivariate:\nf(out, a) = Dir(out|a)\n          = Γ(Σ_i a_i)/(Π_i Γ(a_i)) Π_i out_i^{a_i}\nwhere 'a' is a vector with every a_i > 0\n\nMatrix variate:\nf(out, a) = Π_k Dir(out|a_*k)\nwhere 'a' represents a left-stochastic matrix with every a_jk > 0\n\nInterfaces:\n\n1. out\n2. a\n\nConstruction:\n\nDirichlet(id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.DotProduct","page":"User API","title":"ForneyLab.DotProduct","text":"Description:\n\nout = in1'*in2\n\nin1: d-dimensional vector\nin2: d-dimensional vector\nout: scalar\n\n       in2\n       |\n  in1  V   out\n----->[⋅]----->\n\nf(out, in1, in2) =  δ(out - in1'*in2)\n\nInterfaces:\n\n1 i[:out], 2 i[:in1], 3 i[:in2]\n\nConstruction:\n\nDotProduct(out, in1, in2, id=:my_node)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Equality","page":"User API","title":"ForneyLab.Equality","text":"Description:\n\nAn equality constraint factor node\n\nf([1],[2],[3]) = δ([1] - [2]) δ([1] - [3])\n\nInterfaces:\n\n1, 2, 3\n\nConstruction:\n\nEquality(id=:some_id)\n\nThe interfaces of an Equality node have to be connected manually.\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Exponential","page":"User API","title":"ForneyLab.Exponential","text":"Description:\n\nMaps a location to a scale parameter by exponentiation\n\nf(out,in1) = δ(out - exp(in1))\n\nInterfaces:\n\n1. out\n2. in1\n\nConstruction:\n\nExponential(out, in1, id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Gamma","page":"User API","title":"ForneyLab.Gamma","text":"Description:\n\nA gamma node with shape-rate parameterization:\n\nf(out,a,b) = Gam(out|a,b) = 1/Γ(a) b^a out^{a - 1} exp(-b out)\n\nInterfaces:\n\n1. out\n2. a (shape)\n3. b (rate)\n\nConstruction:\n\nGamma(out, a, b, id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.GaussianMeanPrecision","page":"User API","title":"ForneyLab.GaussianMeanPrecision","text":"Description:\n\nA Gaussian with mean-precision parameterization:\n\nf(out,m,w) = 𝒩(out|m,w) = (2π)^{-D/2} |w|^{1/2} exp(-1/2 (out - m)' w (out - m))\n\nInterfaces:\n\n1. out\n2. m (mean)\n3. w (precision)\n\nConstruction:\n\nGaussianMeanPrecision(out, m, w, id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.GaussianMeanVariance","page":"User API","title":"ForneyLab.GaussianMeanVariance","text":"Description:\n\nA Gaussian with mean-variance parameterization:\n\nf(out,m,v) = 𝒩(out|m,v) = (2π)^{-D/2} |v|^{-1/2} exp(-1/2 (out - m)' v^{-1} (out - m))\n\nInterfaces:\n\n1. out\n2. m (mean)\n3. v (covariance)\n\nConstruction:\n\nGaussianMeanVariance(out, m, v, id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.GaussianMixture","page":"User API","title":"ForneyLab.GaussianMixture","text":"Description:\n\nA Gaussian mixture with mean-precision parameterization:\n\nf(out, z, m1, w1, m2, w2, ...) = 𝒩(out|m1, w1)^z_1 * 𝒩(out|m2, w2)^z_2 * ...\n\nInterfaces:\n\n1. out\n2. z (switch)\n3. m1 (mean)\n4. w1 (precision)\n5. m2 (mean)\n6. w2 (precision)\n...\n\nConstruction:\n\nGaussianMixture(out, z, m1, w1, m2, w2, ..., id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.GaussianWeightedMeanPrecision","page":"User API","title":"ForneyLab.GaussianWeightedMeanPrecision","text":"Description:\n\nA Gaussian with weighted-mean-precision parameterization:\n\nf(out,xi,w) = 𝒩(out|xi,w) = (2π)^{-D/2} |w|^{1/2} exp(-1/2 xi' w^{-1} xi) exp(-1/2 xi' w xi + out' xi)\n\nInterfaces:\n\n1. out\n2. xi (weighted mean, w*m)\n3. w (precision)\n\nConstruction:\n\nGaussianWeightedMeanPrecision(out, xi, w, id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.LogNormal","page":"User API","title":"ForneyLab.LogNormal","text":"Description:\n\nA log-normal node with location-scale parameterization:\n\nf(out,m,s) = logN(out|m, s) = 1/out (2π s)^{-1/2} exp(-1/(2s) (log(out) - m)^2))\n\nInterfaces:\n\n1. out\n2. m (location)\n3. s (squared scale)\n\nConstruction:\n\nLogNormal(out, m, s, id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Logit","page":"User API","title":"ForneyLab.Logit","text":"Description:\n\nLogit mapping between a real variable in1 ∈ R and a binary variable out ∈ {0, 1}.\n\nf(out, in1, xi) =  Ber(out | σ(in1))\n                >= exp(in1*out) σ(xi) exp[-(in1 + xi)/2 - λ(xi)(in1^2 - xi^2)], where\n           σ(x) =  1/(1 + exp(-x))\n           λ(x) =  (σ(x) - 1/2)/(2*x)\n\nInterfaces:\n\n1. out (binary)\n2. in1 (real)\n3. xi (auxiliary variable)\n\nConstruction:\n\nLogit(out, in1, xi)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.MomentConstraint","page":"User API","title":"ForneyLab.MomentConstraint","text":"Description:\n\nConstraints the marginal of the connected variable to an \nexpectation ∫q(x)g(x)dx = G. The parameter η in the node function \nis actively adapted s.t. the marginal respects the above constraint.\nImplementation according to (van de Laar et al. \"Chance-Constrained\nActive Inference\", MIT Neural Computation, 2021).\n\nf(out) = exp(η g(out))\n\nInterfaces:\n\n1. out\n\nConstruction:\n\nMomentConstraint(out; g=g, G=G, id=:my_node)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Multiplication","page":"User API","title":"ForneyLab.Multiplication","text":"Description:\n\nFor continuous random variables, the multiplication node acts\nas a (matrix) multiplication constraint, with node function\n\nf(out, in1, a) = δ(out - a*in1)\n\nInterfaces:\n\n1. out\n2. in1\n3. a\n\nConstruction:\n\nMultiplication(out, in1, a, id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Nonlinear","page":"User API","title":"ForneyLab.Nonlinear","text":"Description:\n\nNonlinear node modeling a nonlinear relation. Updates for\nthe nonlinear node are computed through the unscented transform (by default), \nimportance sampling, or local linear approximation.\n\nFor more details see \"On Approximate Nonlinear Gaussian Message Passing on\nFactor Graphs\", Petersen et al. 2018.\n\nf(out, in1) = δ(out - g(in1))\n\nInterfaces:\n\n1. out\n2. in1\n\nConstruction:\n\nNonlinear{T}(out, in1; g=g, id=:my_node)\nNonlinear{T}(out, in1; g=g, g_inv=g_inv, id=:my_node)\nNonlinear{T}(out, in1, in2, ...; g=g, id=:my_node)\nNonlinear{T}(out, in1, in2, ...; g=g, g_inv=(g_inv_in1, g_inv_in2, ...), id=:my_node)\nNonlinear{T}(out, in1, in2, ...; g=g, g_inv=(g_inv_in1, nothing, ...), id=:my_node)\n\nwhere T encodes the approximation method: Unscented, Sampling, or Extended.\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.PointMassConstraint","page":"User API","title":"ForneyLab.PointMassConstraint","text":"Description:\n\nConstraints the marginal of the connected variable to a point-mass.\nImplementation according to (Senoz et al. \"Variational Message Passing\nand Local Constraint Manipulation in Factor Graphs\", Entropy, 2021).\n\nInterfaces:\n\n1. out\n\nConstruction:\n\nPointMassConstraint(out; id=:my_node)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Poisson","page":"User API","title":"ForneyLab.Poisson","text":"Description:\n\nPoisson factor node\n\nReal scalars\nl > 0 (rate)\n\nf(out, l) = Poisson(out|l) = 1/(x!) * l^x * exp(-l)\n\nInterfaces:\n\n1. out\n2. l\n\nConstruction:\n\nPoisson(id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Probit","page":"User API","title":"ForneyLab.Probit","text":"Description:\n\nConstrains a continuous, real-valued variable in1 ∈ R with a binary (boolean) variable out ∈ {0, 1} through a probit link function.\n\nf(out, in1) = Ber(out | Φ(in1))\n\nInterfaces:\n\n1. out (binary)\n2. in1 (real)\n\nConstruction:\n\nProbit(out, in1, id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Softmax","page":"User API","title":"ForneyLab.Softmax","text":"Description:\n\nSoftmax mapping between a real variable in1 ∈ R^d and discrete variable out ∈ {0, 1}^d.\n\nf(out, in1, xi, a) = Cat(out_j | exp(in1_j)/Σ_k exp(in1_k)),\nwhere log(Σ_k exp(x_k)) is upper-bounded according to (Bouchard, 2007).\n\nInterfaces:\n\n1. out (discrete)\n2. in1 (real)\n3. xi  (auxiliary variable)\n4. a   (auxiliary variable)\n\nConstruction:\n\nSoftmax(out, in1, xi, a)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Transition","page":"User API","title":"ForneyLab.Transition","text":"Description:\n\nThe transition node models a transition between discrete\nrandom variables, with node function\n\nf(out, in1, a) = Cat(out | a*in1)\n\nWhere a is a left-stochastic matrix (columns sum to one)\n\nInterfaces:\n\n1. out\n2. in1\n3. a\n\nConstruction:\n\nTransition(out, in1, a, id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Wishart","page":"User API","title":"ForneyLab.Wishart","text":"Description:\n\nA Wishart node:\n\nf(out,v,nu) = W(out|v, nu) = B(v, nu) |out|^{(nu - D - 1)/2} exp(-1/2 tr(v^{-1} out))\n\nInterfaces:\n\n1. out\n2. v (scale matrix)\n3. nu (degrees of freedom)\n\nConstruction:\n\nWishart(out, v, nu, id=:some_id)\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#Scheduling","page":"User API","title":"Scheduling","text":"","category":"section"},{"location":"library/user-api/","page":"User API","title":"User API","text":"ForneyLab.MarginalTable\nForneyLab.PosteriorFactorization\nForneyLab.InferenceAlgorithm\nForneyLab.Schedule\nForneyLab.currentInferenceAlgorithm","category":"page"},{"location":"library/user-api/#ForneyLab.MarginalTable","page":"User API","title":"ForneyLab.MarginalTable","text":"A MarginalTable defines the update order for marginal computations.\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.PosteriorFactorization","page":"User API","title":"ForneyLab.PosteriorFactorization","text":"Initialize an empty PosteriorFactorization for sequential construction\n\n\n\n\n\nInitialize a PosteriorFactorization consisting of a single PosteriorFactor for the entire graph\n\n\n\n\n\nConstruct a PosteriorFactorization consisting of one PosteriorFactor for each argument\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.InferenceAlgorithm","page":"User API","title":"ForneyLab.InferenceAlgorithm","text":"An InferenceAlgorithm specifies the computations for the quantities of interest.\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.Schedule","page":"User API","title":"ForneyLab.Schedule","text":"A Schedule defines the update order for message computations.\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.currentInferenceAlgorithm","page":"User API","title":"ForneyLab.currentInferenceAlgorithm","text":"Return currently active InferenceAlgorithm. Create one if there is none.\n\n\n\n\n\n","category":"function"},{"location":"library/user-api/#Algorithm-assembly","page":"User API","title":"Algorithm assembly","text":"","category":"section"},{"location":"library/user-api/","page":"User API","title":"User API","text":"ForneyLab.messagePassingAlgorithm","category":"page"},{"location":"library/user-api/#ForneyLab.messagePassingAlgorithm","page":"User API","title":"ForneyLab.messagePassingAlgorithm","text":"Create a message passing algorithm to infer marginals over a posterior distribution\n\n\n\n\n\n","category":"function"},{"location":"library/user-api/#Algorithm-code-generation","page":"User API","title":"Algorithm code generation","text":"","category":"section"},{"location":"library/user-api/","page":"User API","title":"User API","text":"ForneyLab.algorithmSourceCode","category":"page"},{"location":"library/user-api/#ForneyLab.algorithmSourceCode","page":"User API","title":"ForneyLab.algorithmSourceCode","text":"Generate Julia code for message passing and optional free energy evaluation\n\n\n\n\n\n","category":"function"},{"location":"library/user-api/#Algorithm-execution","page":"User API","title":"Algorithm execution","text":"","category":"section"},{"location":"library/user-api/","page":"User API","title":"User API","text":"ForneyLab.Message\nForneyLab.PointMass\nForneyLab.ProbabilityDistribution","category":"page"},{"location":"library/user-api/#ForneyLab.Message","page":"User API","title":"ForneyLab.Message","text":"Encodes a message, which is a probability distribution with a scaling factor\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.PointMass","page":"User API","title":"ForneyLab.PointMass","text":"PointMass is an abstract type used to describe point mass distributions. It never occurs in a FactorGraph, but it is used as a probability distribution type.\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#ForneyLab.ProbabilityDistribution","page":"User API","title":"ForneyLab.ProbabilityDistribution","text":"Encodes a probability distribution as a FactorFunction of type family with fixed interfaces\n\n\n\n\n\n","category":"type"},{"location":"library/user-api/#Helper","page":"User API","title":"Helper","text":"","category":"section"},{"location":"library/user-api/","page":"User API","title":"User API","text":"Modules = [ForneyLab]\nPrivate = true\nPages = [\"src/helpers.jl\"]\nOrder = [:macro, :module, :constant, :type, :function]","category":"page"},{"location":"library/user-api/#ForneyLab.@symmetrical-Tuple{Expr}","page":"User API","title":"ForneyLab.@symmetrical","text":"@symmetrical `function_definition`\n\nDuplicate a method definition with the order of the first two arguments swapped. This macro is used to duplicate methods that are symmetrical in their first two input arguments, but require explicit definitions for the different argument orders. Example:\n\n@symmetrical function prod!(x, y, z)\n    ...\nend\n\n\n\n\n\n","category":"macro"},{"location":"library/user-api/#ForneyLab.cholinv-Tuple{AbstractMatrix{T} where T}","page":"User API","title":"ForneyLab.cholinv","text":"Matrix inversion using Cholesky decomposition\n\n\n\n\n\n","category":"method"},{"location":"library/user-api/#ForneyLab.ensureMatrix-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:Number","page":"User API","title":"ForneyLab.ensureMatrix","text":"Cast input to a Matrix if necessary\n\n\n\n\n\n","category":"method"},{"location":"library/user-api/#ForneyLab.init-Tuple{Symbol}","page":"User API","title":"ForneyLab.init","text":"Helper function to call dynamically generated init functions\n\n\n\n\n\n","category":"method"},{"location":"library/user-api/#ForneyLab.isApproxEqual-Tuple{Any, Any}","page":"User API","title":"ForneyLab.isApproxEqual","text":"Check if arguments are approximately equal\n\n\n\n\n\n","category":"method"},{"location":"library/user-api/#ForneyLab.isRoundedPosDef-Tuple{AbstractMatrix{Float64}}","page":"User API","title":"ForneyLab.isRoundedPosDef","text":"Checks if input matrix is positive definite. We also perform rounding in order to prevent floating point precision problems that isposdef()` suffers from.\n\n\n\n\n\n","category":"method"},{"location":"library/user-api/#ForneyLab.labsbeta-Tuple{Number, Number}","page":"User API","title":"ForneyLab.labsbeta","text":"Wrapper for logabsbeta function that returns first element of its output\n\n\n\n\n\n","category":"method"},{"location":"library/user-api/#ForneyLab.labsgamma-Tuple{Number}","page":"User API","title":"ForneyLab.labsgamma","text":"Wrapper for logabsgamma function that returns first element of its output\n\n\n\n\n\n","category":"method"},{"location":"library/user-api/#ForneyLab.leaftypes-Tuple{Type}","page":"User API","title":"ForneyLab.leaftypes","text":"leaftypes(datatype) returns all subtypes of datatype that are leafs in the type tree.\n\n\n\n\n\n","category":"method"},{"location":"library/user-api/#ForneyLab.mat-Tuple{Any}","page":"User API","title":"ForneyLab.mat","text":"Helper function to construct 1x1 Matrix\n\n\n\n\n\n","category":"method"},{"location":"library/user-api/#ForneyLab.step!-Tuple{Symbol, Vararg{Any, N} where N}","page":"User API","title":"ForneyLab.step!","text":"Helper function to call dynamically generated step! functions\n\n\n\n\n\n","category":"method"},{"location":"library/user-api/#ForneyLab.trigammaInverse-Tuple{Float64}","page":"User API","title":"ForneyLab.trigammaInverse","text":"Solve trigamma(y) = x for y.\n\nUses Newton's method on the convex function 1/trigramma(y). Iterations converge monotonically. Based on trigammaInverse implementation in R package \"limma\" by Gordon Smyth: https://github.com/Bioconductor-mirror/limma/blob/master/R/fitFDist.R\n\n\n\n\n\n","category":"method"}]
}
